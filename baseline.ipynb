{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47f230f-1253-445e-9f2f-c8bac4edeaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31788324, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_dat</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>price</th>\n",
       "      <th>sales_channel_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0663713001</td>\n",
       "      <td>0.050831</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>0541518023</td>\n",
       "      <td>0.030492</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0505221004</td>\n",
       "      <td>0.015237</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0685687003</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-09-20</td>\n",
       "      <td>00007d2de826758b65a93dd24ce629ed66842531df6699...</td>\n",
       "      <td>0685687004</td>\n",
       "      <td>0.016932</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        t_dat                                        customer_id  article_id  \\\n",
       "0  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0663713001   \n",
       "1  2018-09-20  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...  0541518023   \n",
       "2  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0505221004   \n",
       "3  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687003   \n",
       "4  2018-09-20  00007d2de826758b65a93dd24ce629ed66842531df6699...  0685687004   \n",
       "\n",
       "      price  sales_channel_id  \n",
       "0  0.050831                 2  \n",
       "1  0.030492                 2  \n",
       "2  0.015237                 2  \n",
       "3  0.016932                 2  \n",
       "4  0.016932                 2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"transactions_train.csv\", dtype={\"article_id\": str})\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676bba6a-52ca-4ee7-9fa4-f454ec6b27b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-09-22 00:00:00')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"t_dat\"] = pd.to_datetime(df[\"t_dat\"])\n",
    "df[\"t_dat\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3507fd70-b66c-4d6e-919c-33b12a0e646d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72581, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_articles = df.groupby(\"article_id\")[\"t_dat\"].max().reset_index()\n",
    "active_articles = active_articles[active_articles[\"t_dat\"] >= \"2019-09-01\"].reset_index()\n",
    "active_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9bd70ca-14c8-4c32-bee5-361194cb7c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29634404, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"article_id\"].isin(active_articles[\"article_id\"])].reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4251962c-ebac-40ca-b03d-bc1334e21866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "week\n",
       "65     620104\n",
       "13     549443\n",
       "42     518403\n",
       "12     517428\n",
       "64     508664\n",
       "        ...  \n",
       "93     174190\n",
       "102    164298\n",
       "104    163143\n",
       "97     162580\n",
       "94     152807\n",
       "Name: count, Length: 105, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"week\"] = (df[\"t_dat\"].max() - df[\"t_dat\"]).dt.days // 7\n",
    "df[\"week\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13b7aef-bc6f-4cbb-8f54-d4fd2fd2e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.3.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.5.0 scipy-1.13.1 threadpoolctl-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cb54d02-4bf1-4696-b244-fe18df13ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "article_ids = np.concatenate([[\"placeholder\"], np.unique(df[\"article_id\"].values)])\n",
    "\n",
    "le_article = LabelEncoder()\n",
    "le_article.fit(article_ids)\n",
    "df[\"article_id\"] = le_article.transform(df[\"article_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f2ab8e7-8733-45ea-a7da-91cc602fb2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300129, 5), (68984, 5))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEEK_HIST_MAX = 5\n",
    "\n",
    "def create_dataset(df, week):\n",
    "    hist_df = df[(df[\"week\"] > week) & (df[\"week\"] <= week + WEEK_HIST_MAX)]\n",
    "    hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list, \"week\": list}).reset_index()\n",
    "    hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n",
    "\n",
    "    target_df = df[df[\"week\"] == week]\n",
    "    target_df = target_df.groupby(\"customer_id\").agg({\"article_id\": list}).reset_index()\n",
    "    target_df.rename(columns={\"article_id\": \"target\"}, inplace=True)\n",
    "    target_df[\"week\"] = week\n",
    "\n",
    "    return target_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "val_weeks = [0]\n",
    "train_weeks = [1, 2, 3, 4]\n",
    "\n",
    "\n",
    "val_df = pd.concat([create_dataset(df, w) for w in val_weeks]).reset_index(drop=True)\n",
    "train_df = pd.concat([create_dataset(df, w) for w in train_weeks]).reset_index(drop=True)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70ab1b99-f49f-4616-8357-d9921fce3522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m986.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5c6617-87ab-476a-9cb4-10147986b646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0, 70912, 69932, 70149, 14648,\n",
       "         11949, 66254, 66254, 67809]),\n",
       " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 0.2000, 0.2000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class HMDataset(Dataset):\n",
    "    def __init__(self, df, seq_len, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.seq_len = seq_len\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "\n",
    "        if self.is_test:\n",
    "            target = torch.zeros(2).float()\n",
    "        else:\n",
    "            target = torch.zeros(len(article_ids)).float()\n",
    "            for t in row.target:\n",
    "                target[t] = 1.0\n",
    "\n",
    "        article_hist = torch.zeros(self.seq_len).long()\n",
    "        week_hist = torch.ones(self.seq_len).float()\n",
    "\n",
    "\n",
    "        if isinstance(row.article_id, list):\n",
    "            if len(row.article_id) >= self.seq_len:\n",
    "                article_hist = torch.LongTensor(row.article_id[-self.seq_len:])\n",
    "                week_hist = (torch.LongTensor(row.week_history[-self.seq_len:]) - row.week)/WEEK_HIST_MAX/2\n",
    "            else:\n",
    "                article_hist[-len(row.article_id):] = torch.LongTensor(row.article_id)\n",
    "                week_hist[-len(row.article_id):] = (torch.LongTensor(row.week_history) - row.week)/WEEK_HIST_MAX/2\n",
    "\n",
    "        return article_hist, week_hist, target\n",
    "\n",
    "HMDataset(val_df, 64)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e79bdc8-fd05-4cb9-aae5-28f6d02a1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch):\n",
    "    if epoch < 1:\n",
    "        lr = 5e-5\n",
    "    elif epoch < 6:\n",
    "        lr = 1e-3\n",
    "    elif epoch < 9:\n",
    "        lr = 1e-4\n",
    "    else:\n",
    "        lr = 1e-5\n",
    "\n",
    "    for p in optimizer.param_groups:\n",
    "        p['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def get_optimizer(net):\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=3e-4, betas=(0.9, 0.999),\n",
    "                                 eps=1e-08)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a3e1028-8d7a-4744-9e2b-9c0ffffa8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HMModel(nn.Module):\n",
    "    def __init__(self, article_shape):\n",
    "        super(HMModel, self).__init__()\n",
    "\n",
    "        self.article_emb = nn.Embedding(article_shape[0], embedding_dim=article_shape[1])\n",
    "\n",
    "        self.article_likelihood = nn.Parameter(torch.zeros(article_shape[0]), requires_grad=True)\n",
    "        self.top = nn.Sequential(nn.Conv1d(3, 32, kernel_size=1), nn.LeakyReLU(),\n",
    "                                 nn.Conv1d(32, 8, kernel_size=1), nn.LeakyReLU(),\n",
    "                                 nn.Conv1d(8, 1, kernel_size=1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        article_hist, week_hist = inputs[0], inputs[1]\n",
    "        x = self.article_emb(article_hist)\n",
    "        x = F.normalize(x, dim=2)\n",
    "\n",
    "        x = x@F.normalize(self.article_emb.weight).T\n",
    "\n",
    "        x, indices = x.max(axis=1)\n",
    "        x = x.clamp(1e-3, 0.999)\n",
    "        x = -torch.log(1/x - 1)\n",
    "\n",
    "        max_week = week_hist.unsqueeze(2).repeat(1, 1, x.shape[-1]).gather(1, indices.unsqueeze(1).repeat(1, week_hist.shape[1], 1))\n",
    "        max_week = max_week.mean(axis=1).unsqueeze(1)\n",
    "\n",
    "        x = torch.cat([x.unsqueeze(1), max_week,\n",
    "                       self.article_likelihood[None, None, :].repeat(x.shape[0], 1, 1)], axis=1)\n",
    "\n",
    "        x = self.top(x).squeeze(1)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = HMModel((len(le_article.classes_), 512))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c456efa1-d9e3-4934-9219-df6dd87f3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def calc_map(topk_preds, target_array, k=12):\n",
    "    metric = []\n",
    "    tp, fp = 0, 0\n",
    "\n",
    "    for pred in topk_preds:\n",
    "        if target_array[pred]:\n",
    "            tp += 1\n",
    "            metric.append(tp/(tp + fp))\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    return np.sum(metric) / min(k, target_array.sum())\n",
    "\n",
    "def read_data(data):\n",
    "    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n",
    "\n",
    "\n",
    "def validate(model, val_loader, k=12):\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "\n",
    "    maps = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "\n",
    "            logits = model(inputs)\n",
    "\n",
    "            _, indices = torch.topk(logits, k, dim=1)\n",
    "\n",
    "            indices = indices.detach().cpu().numpy()\n",
    "            target = target.detach().cpu().numpy()\n",
    "\n",
    "            for i in range(indices.shape[0]):\n",
    "                maps.append(calc_map(indices[i], target[i]))\n",
    "\n",
    "\n",
    "    return np.mean(maps)\n",
    "\n",
    "SEQ_LEN = 16\n",
    "\n",
    "BS = 32\n",
    "NW = 2\n",
    "\n",
    "val_dataset = HMDataset(val_df, SEQ_LEN)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                          pin_memory=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f905add1-da13-4a29-b3c2-e4d1a9ce0fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 99.4574 lr: 5e-05: 100%|██████████████████████████████████████████████| 9379/9379 [06:46<00:00, 23.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 46.11it/s]\n",
      "Epoch 1\n",
      "Train Loss: 99.4574\n",
      "Validation MAP: 0.023463344107770078\n",
      "\n",
      "Epoch 2 Loss: 99.3079 lr: 0.001: 100%|██████████████████████████████████████████████| 9379/9379 [06:36<00:00, 23.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 46.82it/s]\n",
      "Epoch 2\n",
      "Train Loss: 99.3079\n",
      "Validation MAP: 0.023983598739893707\n",
      "\n",
      "Epoch 3 Loss: 99.2424 lr: 0.001: 100%|██████████████████████████████████████████████| 9379/9379 [06:36<00:00, 23.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:48<00:00, 44.64it/s]\n",
      "Epoch 3\n",
      "Train Loss: 99.2424\n",
      "Validation MAP: 0.02311642030903419\n",
      "\n",
      "Epoch 4 Loss: 99.2212 lr: 0.001: 100%|██████████████████████████████████████████████| 9379/9379 [06:38<00:00, 23.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 46.24it/s]\n",
      "Epoch 4\n",
      "Train Loss: 99.2212\n",
      "Validation MAP: 0.023058467315314645\n",
      "\n",
      "Epoch 5 Loss: 99.2004 lr: 0.001: 100%|██████████████████████████████████████████████| 9379/9379 [06:45<00:00, 23.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:47<00:00, 45.09it/s]\n",
      "Epoch 5\n",
      "Train Loss: 99.2004\n",
      "Validation MAP: 0.023266210282059687\n",
      "\n",
      "Epoch 6 Loss: 99.1767 lr: 0.001: 100%|██████████████████████████████████████████████| 9379/9379 [08:11<00:00, 19.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [01:03<00:00, 34.11it/s]\n",
      "Epoch 6\n",
      "Train Loss: 99.1767\n",
      "Validation MAP: 0.022830309336845347\n",
      "\n",
      "Epoch 7 Loss: 99.1297 lr: 0.0001: 100%|█████████████████████████████████████████████| 9379/9379 [08:31<00:00, 18.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [01:01<00:00, 34.94it/s]\n",
      "Epoch 7\n",
      "Train Loss: 99.1297\n",
      "Validation MAP: 0.022518378368877167\n",
      "\n",
      "Epoch 8 Loss: 99.1204 lr: 0.0001: 100%|█████████████████████████████████████████████| 9379/9379 [07:01<00:00, 22.27it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 45.99it/s]\n",
      "Epoch 8\n",
      "Train Loss: 99.1204\n",
      "Validation MAP: 0.022430095129194638\n",
      "\n",
      "Epoch 9 Loss: 99.1108 lr: 0.0001: 100%|█████████████████████████████████████████████| 9379/9379 [06:52<00:00, 22.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 46.15it/s]\n",
      "Epoch 9\n",
      "Train Loss: 99.1108\n",
      "Validation MAP: 0.02261133131103963\n",
      "\n",
      "Epoch 10 Loss: 99.0993 lr: 1e-05: 100%|█████████████████████████████████████████████| 9379/9379 [06:50<00:00, 22.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:46<00:00, 46.03it/s]\n",
      "Epoch 10\n",
      "Train Loss: 99.0993\n",
      "Validation MAP: 0.022389141779356737\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dice_loss(y_pred, y_true):\n",
    "    y_pred = y_pred.sigmoid()\n",
    "    intersect = (y_true*y_pred).sum(axis=1)\n",
    "\n",
    "    return 1 - (intersect/(intersect + y_true.sum(axis=1) + y_pred.sum(axis=1))).mean()\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    optimizer = get_optimizer(model)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, file=sys.stdout)\n",
    "\n",
    "        lr = adjust_lr(optimizer, e)\n",
    "\n",
    "        loss_list = []\n",
    "\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, target) + dice_loss(logits, target)\n",
    "\n",
    "\n",
    "            #loss.backward()\n",
    "            scaler.scale(loss).backward()\n",
    "            #optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "\n",
    "            avg_loss = np.round(100*np.mean(loss_list), 4)\n",
    "\n",
    "            tbar.set_description(f\"Epoch {e+1} Loss: {avg_loss} lr: {lr}\")\n",
    "\n",
    "        val_map = validate(model, val_loader)\n",
    "\n",
    "        log_text = f\"Epoch {e+1}\\nTrain Loss: {avg_loss}\\nValidation MAP: {val_map}\\n\"\n",
    "\n",
    "        print(log_text)\n",
    "\n",
    "        #logfile = open(f\"models/{MODEL_NAME}_{SEED}.txt\", 'a')\n",
    "        #logfile.write(log_text)\n",
    "        #logfile.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "MODEL_NAME = \"exp001\"\n",
    "SEED = 0\n",
    "\n",
    "train_dataset = HMDataset(train_df, SEQ_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=NW,\n",
    "                          pin_memory=True, drop_last=True)\n",
    "\n",
    "model = train(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "696ed250-d7bf-44b0-b4f8-24d35687268b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 99.1118 lr: 5e-05: 100%|██████████████████████████████████████████████| 9283/9283 [08:12<00:00, 18.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.64it/s]\n",
      "Epoch 1\n",
      "Train Loss: 99.1118\n",
      "Validation MAP: 0.02281379879790295\n",
      "\n",
      "Epoch 2 Loss: 99.1198 lr: 0.001: 100%|██████████████████████████████████████████████| 9283/9283 [08:26<00:00, 18.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.53it/s]\n",
      "Epoch 2\n",
      "Train Loss: 99.1198\n",
      "Validation MAP: 0.02518466185549604\n",
      "\n",
      "Epoch 3 Loss: 99.0855 lr: 0.001: 100%|██████████████████████████████████████████████| 9283/9283 [08:27<00:00, 18.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.53it/s]\n",
      "Epoch 3\n",
      "Train Loss: 99.0855\n",
      "Validation MAP: 0.025543896871232236\n",
      "\n",
      "Epoch 4 Loss: 99.0593 lr: 0.001: 100%|██████████████████████████████████████████████| 9283/9283 [08:27<00:00, 18.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:52<00:00, 41.41it/s]\n",
      "Epoch 4\n",
      "Train Loss: 99.0593\n",
      "Validation MAP: 0.02601509394211474\n",
      "\n",
      "Epoch 5 Loss: 99.0456 lr: 0.001: 100%|██████████████████████████████████████████████| 9283/9283 [07:52<00:00, 19.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:49<00:00, 43.43it/s]\n",
      "Epoch 5\n",
      "Train Loss: 99.0456\n",
      "Validation MAP: 0.026751584260624288\n",
      "\n",
      "Epoch 6 Loss: 99.0308 lr: 0.001: 100%|██████████████████████████████████████████████| 9283/9283 [07:32<00:00, 20.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:47<00:00, 45.56it/s]\n",
      "Epoch 6\n",
      "Train Loss: 99.0308\n",
      "Validation MAP: 0.02592002799168003\n",
      "\n",
      "Epoch 7 Loss: 98.9812 lr: 0.0001: 100%|█████████████████████████████████████████████| 9283/9283 [07:10<00:00, 21.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:44<00:00, 48.27it/s]\n",
      "Epoch 7\n",
      "Train Loss: 98.9812\n",
      "Validation MAP: 0.02676772710276785\n",
      "\n",
      "Epoch 8 Loss: 98.9712 lr: 0.0001: 100%|█████████████████████████████████████████████| 9283/9283 [08:22<00:00, 18.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.64it/s]\n",
      "Epoch 8\n",
      "Train Loss: 98.9712\n",
      "Validation MAP: 0.026881899865073854\n",
      "\n",
      "Epoch 9 Loss: 98.9627 lr: 0.0001: 100%|█████████████████████████████████████████████| 9283/9283 [08:20<00:00, 18.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.72it/s]\n",
      "Epoch 9\n",
      "Train Loss: 98.9627\n",
      "Validation MAP: 0.02702504391570144\n",
      "\n",
      "Epoch 10 Loss: 98.9523 lr: 1e-05: 100%|█████████████████████████████████████████████| 9283/9283 [08:20<00:00, 18.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 2156/2156 [00:51<00:00, 41.51it/s]\n",
      "Epoch 10\n",
      "Train Loss: 98.9523\n",
      "Validation MAP: 0.026738734314082104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.concat([train_df[train_df[\"week\"] < 4], val_df])\n",
    "train_dataset = HMDataset(combined_df, SEQ_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=NW,\n",
    "                          pin_memory=False, drop_last=True)\n",
    "model = train(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49cc27aa-72e7-4e00-a992-9df2fcd9b2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1371980, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...\n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...\n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...\n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...\n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f..."
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('sample_submission.csv').drop(\"prediction\", axis=1)\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ff90d5dc-ad6b-4ffb-bc34-ce9a9edf18ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>week</th>\n",
       "      <th>article_id</th>\n",
       "      <th>week_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[7154]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[46435]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_id  week article_id  \\\n",
       "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...    -1     [7154]   \n",
       "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...    -1        NaN   \n",
       "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...    -1    [46435]   \n",
       "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...    -1        NaN   \n",
       "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...    -1        NaN   \n",
       "\n",
       "  week_history  \n",
       "0          [2]  \n",
       "1          NaN  \n",
       "2          [1]  \n",
       "3          NaN  \n",
       "4          NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_test_dataset(test_df):\n",
    "    week = -1\n",
    "    test_df[\"week\"] = week\n",
    "\n",
    "    hist_df = df[(df[\"week\"] > week) & (df[\"week\"] <= week + WEEK_HIST_MAX)]\n",
    "    hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list, \"week\": list}).reset_index()\n",
    "    hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n",
    "\n",
    "\n",
    "    return test_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "test_df = create_test_dataset(test_df)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fda775d6-c994-4ef4-a620-55df6ec15502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8008965145264508"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"article_id\"].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb0ec74f-7f96-4a96-9727-942dac7b6eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 42875/42875 [26:59<00:00, 26.48it/s]\n"
     ]
    }
   ],
   "source": [
    "test_ds = HMDataset(test_df, SEQ_LEN, is_test=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BS, shuffle=False, num_workers=NW,\n",
    "                          pin_memory=False, drop_last=False)\n",
    "\n",
    "\n",
    "def inference(model, loader, k=12):\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(loader, file=sys.stdout)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            inputs, target = read_data(data)\n",
    "\n",
    "            logits = model(inputs)\n",
    "\n",
    "            _, indices = torch.topk(logits, k, dim=1)\n",
    "\n",
    "            indices = indices.detach().cpu().numpy()\n",
    "            target = target.detach().cpu().numpy()\n",
    "\n",
    "            for i in range(indices.shape[0]):\n",
    "                preds.append(\" \".join(list(le_article.inverse_transform(indices[i]))))\n",
    "\n",
    "\n",
    "    return preds\n",
    "\n",
    "\n",
    "test_df[\"prediction\"] = inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0103f0f8-e404-4adb-94e4-71c1bf69d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"submission.csv\", index=False, columns=[\"customer_id\", \"prediction\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
